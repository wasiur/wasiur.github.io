I organise the Statistics and Probability seminars at Nottingham Maths. Below is the list of upcoming talks in the seminar series. 

## Autumn 2022
<ul>
  <li>06 October 2022, 2pm. <a href="https://sites.harvard.edu/prs499/">Pragya Sur</a>, Harvard University.
    <ul>
      <li><b>Title:</b>  <em> A New Perspective on High-Dimensional Causal Inference</em></li>
      <li>
        <details>
          <summary>
            <b>Abstract:</b>
          </summary>
          <em>Causal inference from high-dimensional observational studies poses intriguing challenges. In this context, the augmented inverse probability weighting estimator is widely used for average treatment effect estimation. This estimator exhibits fascinating properties, such as double robustness. However, existing statistical guarantees rely on some form of sparsity in the underlying model, and may fail to apply in practical settings when these assumptions are  violated. In this talk, we present a new central limit theorem for this estimator, that applies in high dimensions, without sparsity-type assumptions on underlying signals. Specifically, we work in the proportional asymptotics regime, where the number of features and samples are both large and comparable. Our work uncovers novel  high-dimensional phenomena that are strikingly different from their classical counterparts. To conclude, we discuss opportunities that arise in our framework, when modern machine-learning-based estimators are used for learning the  high-dimensional nuisance parameters.  On the technical front, our work utilizes a novel interplay between three distinct tools---the theory of deterministic equivalents, approximate message passing theory, and the leave-one-out approach (alternately known as the cavity method in statistical physics). 
          <p>This is based on joint work with Kuanhao Jiang, Rajarshi Mukherjee, and Subhabrata Sen (Harvard).</p></em>
        </details>
      </li>
    </ul>
  </li>
  <li>13 October 2022, 2pm. <a href="https://aliceschwarze.gitlab.io/">Alice Schwarze</a>, Dartmouth College.
    <ul>
      <li><b>Title:</b>  <em>  Network inference via process motifs for lagged correlation in linear stochastic processes </em></li>
      <li>
        <details>
          <summary><b>Abstract:</b></summary>
               <em>A major challenge for causal inference from time-series data is the trade-off between computational feasibility and accuracy. Motivated by process motifs for lagged covariance in an autoregressive model with slow mean-reversion, we propose to infer networks of causal relations via pairwise edge measures (PEMs) that one can easily compute from lagged correlation matrices, and we formulate two PEMs that respectively correct for confounding factors and for reverse causation. To demonstrate the performance of our PEMs, we consider network inference from simulations of linear stochastic processes, and we show that our proposed PEMs can infer networks accurately and efficiently. Specifically, for autocorrelated time-series data, our approach achieves accuracies higher than or similar to Granger causality, transfer entropy, and convergent crossmapping---but with much shorter computation time than possible with any of these methods. Our fast and accurate PEMs are easy-to-implement methods for network inference with a clear theoretical underpinning. They provide promising alternatives to current paradigms for the inference of linear models from time-series data, including Granger causality, vector-autoregression, and sparse inverse covariance estimation.</em>
        </details>
      </li>
    </ul>
  </li>
  <li>20 October 2022, 2pm. <a href="https://rcorradin.github.io/">Riccardo Corradin</a>, University of Nottingham.
    <ul>
      <li><b>Title:</b> <em>  A journey through model-based clustering with intractable distributions </em></li>
      <li>
        <details>
          <summary><b>Abstract:</b> </summary>
            <em>Model-based clustering represents one of the fundamental procedures in a statistician's toolbox. Within the model-based clustering framework, we consider the case where the kernel distribution of nonparametric mixture models is available only up to an intractable normalizing constant, in which most of the commonly used Markov chain Monte Carlo methods fail to provide posterior inference. To overcome this problem, we propose an approximate Bayesian computational strategy, whereby we approximate the posterior to avoid the intractability of the kernel. By exploiting the structure of the nonparametric prior, our proposal combines the use of predictive distributions as a proposal with transport maps to obtain an efficient and flexible sampling strategy. Further, we illustrate how the specification of our proposal can be relaxed by introducing an adaptive scheme on the degree of approximation of the posterior distribution. Empirical evidence from simulation studies shows that our proposal outperforms its main competitors in terms of computational times while preserving comparable accuracy of the estimates.</em>
        </details>
      </li>
    </ul>
  </li>
  <li>27 October 2022, 2pm. <a href="https://veberamandine.wixsite.com/maths">Amandine Veber</a>, Universite Paris Cite.
    <ul>
      <li><b>Title:</b> <em>  Growth properties of the infite-parent spatial Lambda-Fleming-Viot process</em></li>
      <li>
        <details>
          <summary> <b>Abstract:</b></summary>
            <em>The infinite-parent  spatial Lambda-Fleming-Viot process is a model for spatially expanding populations in a two dimensional continuum, in which empty areas are filled with ghost individuals. This model can be seen as a continuous-space version of the Eden growth model, and it comes with a dual process that allows us to trace back the origins of a sample of individuals taken from the current population. In this talk, we shall focus on the growth properties of the area covered by real individuals. With the help of a simple toy model, we shall also investigate how the fluctuations at the front edge lead to a much larger speed of growth of the occupied region than that predicted by simple first-moment estimates.
            <p>Joint work with Apolline Louvet (Ecole Polytechnique and University Paris Cité, and soon University of Bath) </p>  </em>     
          </details>
      </li>
    </ul>
  </li>
  <li>3 November 2022, 2pm. <a href="https://acms.nd.edu/people/lizhen-lin/">Lizhen Lin</a>, University of Notre Dame.
    <ul>
      <li><b>Title:</b> <em>  Adaptive variational Bayes: Optimality, computation and applications</em> </li>
      <li>
        <details>
          <summary><b>Abstract:</b></summary>
             <em>In this talk, I'll discuss adaptive  statistical inference based on variational Bayes. Although a number of studies have been conducted to analyze theoretical properties such as posterior contraction properties of variational posteriors, there is still a lack of general and computationally tractable variational Bayes methods that can achieve adaptive inference. To fill this gap, we propose a novel adaptive variational Bayes framework, which can operate on a collection of models.  The proposed framework first computes a variational posterior over each individual model separately and then combines them with certain weights to produce a variational posterior over the entire model. It turns out that this combined variational posterior is the closest member to the posterior over the entire model in a predefined family of approximating distributions. We show that the proposed variational posterior achieves optimal contraction rates adaptively under very general conditions and attains model selection consistency when the true model structure exists. We apply the general results obtained for the adaptive variational Bayes to a large class of statistical models  including deep learning models and derive some new and adaptive inference results.</em>
          </details>
      </li>
    </ul>
  </li>
  <li>10 November 2022, 2pm. <a href="https://www.nottingham.ac.uk/mathematics/people/aidan.o'keeffe">Aidan O'Keeffe</a>, University of Nottingham.
    <ul>
      <li><b>Title:</b> <em>  TBA</em></li>
      <li>
        <details>
          <summary> <b>Abstract:</b> </summary>
          <em>  TBA</em>
        </details>
      </li>
    </ul>
  </li>
  <li>17 November 2022, 2pm. <a href="https://sites.google.com/view/sylvie-meleard/accueil">Sylvie Méléard</a>, Ecole Polytechnique.
    <ul>
      <li><b>Title:</b> <em>  TBA</em> </li>
      <li>
        <details>
          <summary> <b>Abstract:</b> </summary>
         <em>  TBA</em>
        </details>
      </li>
    </ul>
  </li>
  <li>24 November 2022, 12pm. <a href="https://www.nottingham.ac.uk/mathematics/people/luis.espath">Luis Espath</a>, University of Nottingham.
    <ul>
      <li>
        <details>
          <summary><b>Title:</b> <em>  TBA</em> </summary>
          <b>Abstract:</b> <em>  TBA</em>
        </details>
      </li>
    </ul>
  </li>
  <li>1 December 2022, 4pm. <a href="https://www.kcl.ac.uk/people/maria-kalli">Maria Kalli</a>, King's College.
    <ul>
      <li><b>Title:</b> <em>  TBA</em></li>
      <li>
        <details>
          <summary> <b>Abstract:</b> </summary>
           <em>  TBA</em>
        </details>
      </li>
    </ul>
  </li>
  <li>8 December 2022, 12pm. <a href="https://sites.google.com/view/rebecca-chisholm/home">Rebecca Chisholm</a>, La Trobe University.
    <ul>
      <li><b>Title:</b> <em>  TBA</em></li>
      <li>
        <details>
          <summary> <b>Abstract:</b> </summary>
          <em>  TBA</em>
        </details>
      </li>
    </ul>
  </li>
</ul> 

## Spring 2023
<ul>
  <li>6 April 2023, 2pm. <a href="https://fadikar.com/about/">Arindam Fadikar</a>, Argonne National Laboratory.
    <ul>
      <li> <b>Title:</b> <em>  TBA</em></li>
      <li>
        <details>
          <summary> <b>Abstract:</b> </summary>
           <em>  TBA</em>
        </details>
      </li>
    </ul>
  </li>
</ul>